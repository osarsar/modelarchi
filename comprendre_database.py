import os
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM

# ===============================
# CONFIG
# ===============================
os.environ["CUDA_VISIBLE_DEVICES"] = ""  # enlever si GPU Toubkal
torch.set_num_threads(4)

DOCS_PATH = "../index/map_docs.npy"

EMBED_MODEL = "intfloat/multilingual-e5-large"
LLM_MODEL = "Qwen/Qwen2.5-1.5B-Instruct"

TOP_K = 20          # on prend large
FINAL_KEEP = 8      # aprÃ¨s raisonnement
SIM_THRESHOLD = 0.65


# ===============================
# LOAD
# ===============================
print("\nğŸ§  [INIT] Chargement des modÃ¨les...")

print("ğŸ”¹ Chargement encodeur sÃ©mantique (E5)")
encoder = SentenceTransformer(EMBED_MODEL, device="cpu")
print("   âœ” Encodeur chargÃ©")

print("ğŸ”¹ Chargement tokenizer LLM")
tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)

print("ğŸ”¹ Chargement modÃ¨le LLM (raisonnement humain)")
llm = AutoModelForCausalLM.from_pretrained(
    LLM_MODEL,
    device_map="cpu"
).eval()
print("   âœ” LLM prÃªt")

print("ğŸ”¹ Chargement base de donnÃ©es titres")
docs = np.load(DOCS_PATH, allow_pickle=True)
print(f"   âœ” {len(docs)} documents chargÃ©s")


# ===============================
# STEP 1 â€” SEMANTIC FILTER
# ===============================
def retrieve_candidates(news):
    print("\nğŸ” [STEP 1] Recherche sÃ©mantique sur les TITRES")
    print("ğŸ“° News entrÃ©e :", news)

    q = encoder.encode(
        f"query: {news}",
        normalize_embeddings=True
    )

    candidates = []

    for idx, d in enumerate(docs):
        title = d.get("title", "")
        if not title:
            continue

        emb = encoder.encode(
            f"passage: {title}",
            normalize_embeddings=True
        )

        score = float(np.dot(q, emb))

        if score >= SIM_THRESHOLD:
            print(f"   âœ MATCH POTENTIEL [{idx}]")
            print(f"      Titre : {title}")
            print(f"      Score : {round(score, 3)}")
            candidates.append((score, title))

    candidates.sort(reverse=True, key=lambda x: x[0])

    print(f"\nğŸ“Š Total candidats retenus (score â‰¥ {SIM_THRESHOLD}) : {len(candidates)}")
    print(f"ğŸ“Œ Top {TOP_K} conservÃ©s pour raisonnement humain")

    return candidates[:TOP_K]


# ===============================
# STEP 2 â€” HUMAN-LIKE REASONING
# ===============================
def same_information(news, title):
    print("\nğŸ§  [STEP 2] Raisonnement humain (LLM)")
    print("ğŸ”¹ Comparaison :")
    print("   A (news)  :", news)
    print("   B (titre) :", title)

    prompt = f"""
Question trÃ¨s prÃ©cise :

Titre A (news reÃ§ue) :
"{news}"

Titre B (base de donnÃ©es) :
"{title}"

Question :
Ces deux titres parlent-ils de la MÃŠME INFORMATION FACTUELLE ?

RÃ©ponds uniquement par :
- OUI : si c'est exactement la mÃªme information
- NON : sinon

RÃ©ponse :
""".strip()

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512
    )

    with torch.no_grad():
        out = llm.generate(
            **inputs,
            max_new_tokens=5
        )

    answer = tokenizer.decode(out[0], skip_special_tokens=True).upper()

    decision = "OUI" if "OUI" in answer else "NON"

    print("ğŸ§¾ RÃ©ponse brute LLM :", answer)
    print("â¡ï¸ DÃ©cision finale :", decision)

    return decision == "OUI"


# ===============================
# MAIN
# ===============================
def main():
    print("\nğŸ“° [INPUT] Donne une news :")
    news = input("> ").strip()

    print("\nğŸš€ DÃ‰MARRAGE PIPELINE DE LOCALISATION DE Lâ€™INFORMATION")
    print("=" * 80)

    # ---- STEP 1
    candidates = retrieve_candidates(news)

    if not candidates:
        print("\nâŒ Aucun titre candidat trouvÃ©")
        return

    print("\nğŸ§  [STEP 2] Validation humaine des candidats\n")

    confirmed = []

    for i, (score, title) in enumerate(candidates, 1):
        print(f"\nğŸ” CANDIDAT #{i}")
        print(f"ğŸ“Œ Titre : {title}")
        print(f"ğŸ“Š Score sÃ©mantique : {round(score, 3)}")

        ok = same_information(news, title)

        if ok:
            print("âœ… CONFIRMÃ‰ : mÃªme information")
            confirmed.append(title)
        else:
            print("âŒ REJETÃ‰ : information diffÃ©rente")

        print("-" * 80)

        if len(confirmed) >= FINAL_KEEP:
            print("âš ï¸ Limite atteinte â€” arrÃªt anticipÃ©")
            break

    print("\nğŸ“Œ TITRES DE RÃ‰FÃ‰RENCE (OÃ™ Lâ€™INFO EST PARLÃ‰E)")
    print("=" * 80)

    if not confirmed:
        print("âŒ Aucun titre validÃ© comme rÃ©fÃ©rence fiable")
        return

    for t in confirmed:
        print("â€¢", t)

    print("\nâœ… FIN DU PROCESSUS")


if __name__ == "__main__":
    main()
