import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ===============================
# CONFIG â€” CPU ONLY
# ===============================
os.environ["CUDA_VISIBLE_DEVICES"] = ""
torch.set_num_threads(4)

MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"


SYSTEM_PROMPT = """
Tu es un analyste factuel institutionnel.

Ta tÃ¢che est de dÃ©terminer si DEUX textes parlent de LA MÃŠME INFORMATION FACTUELLE.

IMPORTANT :
- Tu ne dois PAS utiliser de probabilitÃ©s
- Tu ne dois PAS Ãªtre vague
- Tu dois raisonner comme un humain
- Deux textes ne sont "la mÃªme information" QUE SI :
  - le mÃªme fait prÃ©cis est dÃ©crit
  - les mÃªmes acteurs principaux sont impliquÃ©s
  - le mÃªme lieu OU le mÃªme cadre est mentionnÃ©
  - le mÃªme sujet central est traitÃ©

ThÃ¨me proche â‰  mÃªme information.
CompatibilitÃ© â‰  mÃªme information.

Tu dois produire :
1) Une comprÃ©hension du texte A
2) Une comprÃ©hension du texte B
3) Une comparaison factuelle point par point
4) Une dÃ©cision FINALE :
   - MÃŠME INFORMATION
   - INFORMATION DIFFÃ‰RENTE
   - INFORMATION CONTRADICTOIRE
"""


def build_prompt(text_a: str, text_b: str) -> str:
    return f"""
Analyse les deux textes suivants.

TEXTE A :
{text_a}

TEXTE B :
{text_b}

RÃ©ponds STRICTEMENT avec cette structure :

COMPRÃ‰HENSION TEXTE A :
- Fait principal :
- Acteurs :
- Lieu / Cadre :
- Sujet :

COMPRÃ‰HENSION TEXTE B :
- Fait principal :
- Acteurs :
- Lieu / Cadre :
- Sujet :

COMPARAISON FACTUELLE :
- SimilaritÃ©s :
- DiffÃ©rences :

DÃ‰CISION FINALE :
(choisis UNE seule)
- MÃŠME INFORMATION
- INFORMATION DIFFÃ‰RENTE
- INFORMATION CONTRADICTOIRE

JUSTIFICATION :
(raisonnement clair et logique)
""".strip()


def compare_like_human(text_a: str, text_b: str):
    print("ðŸ§  Chargement du modÃ¨le...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        device_map="cpu"
    ).eval()

    prompt = SYSTEM_PROMPT + "\n\n" + build_prompt(text_a, text_b)

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=1800
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=500,
            do_sample=False,
            temperature=0.0
        )

    answer = tokenizer.decode(output[0], skip_special_tokens=True)

    print("\nðŸ§  ANALYSE HUMAINE\n")
    print(answer)


if __name__ == "__main__":
    print("ðŸ“° TEXTE A :")
    text_a = input("> ")

    print("\nðŸ“° TEXTE B :")
    text_b = input("> ")

    compare_like_human(text_a, text_b)
