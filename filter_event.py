import os
import re
from typing import List, Dict, Tuple

import torch
from sentence_transformers import SentenceTransformer, CrossEncoder

from transformers import AutoTokenizer, AutoModelForCausalLM

# =========================
# CONFIG
# =========================
os.environ["CUDA_VISIBLE_DEVICES"] = ""
torch.set_num_threads(4)

# Bi-encoder (multilingue, solide pour retrieval)
BI_ENCODER = "intfloat/multilingual-e5-large"

# Cross-encoder (reranker multilingue) :
# bon choix CPU : relativement l√©ger + performant
CROSS_ENCODER = "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1"

# Optionnel: LLM tie-breaker (seulement si borderline)
USE_LLM_TIEBREAKER = True
LLM_MODEL = "Qwen/Qwen2.5-1.5B-Instruct"

TOP_K_RETRIEVE = 20

# seuils cross-encoder (√† ajuster)
THRESH_ACCEPT = 0.62      # >= -> m√™me √©v√©nement
THRESH_REJECT = 0.45      # <= -> pas m√™me √©v√©nement
# entre les deux -> tie-breaker (LLM) ou rejet prudent


# =========================
# TEXT UTILS
# =========================
def normalize_spaces(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()


def strip_quotes(text: str) -> str:
    return text.replace("¬´", '"').replace("¬ª", '"').replace("‚Äô", "'")


def make_pseudo_title_from_news(news: str, max_len: int = 220) -> str:
    """
    Convertit une phrase news -> pseudo-titre court pour faire du matching sur titres.
    (On garde uniquement l'essentiel du d√©but)
    """
    t = normalize_spaces(strip_quotes(news))
    # enlever guillemets inutiles
    t = re.sub(r"^[-‚Ä¢\s]+", "", t)
    # couper √† une longueur raisonnable
    return t[:max_len]


# =========================
# LOAD MODELS
# =========================
print("üß† Chargement Bi-encoder (retrieval)...")
biencoder = SentenceTransformer(BI_ENCODER, device="cpu")

print("üß† Chargement Cross-encoder (rerank √©v√©nement)...")
cross = CrossEncoder(CROSS_ENCODER, device="cpu")

llm_tok, llm = None, None
if USE_LLM_TIEBREAKER:
    print("üß† Chargement LLM tie-breaker (Qwen)...")
    llm_tok = AutoTokenizer.from_pretrained(LLM_MODEL)
    llm = AutoModelForCausalLM.from_pretrained(LLM_MODEL, device_map="cpu").eval()


# =========================
# RETRIEVE (Bi-encoder)
# =========================
def embed_text_e5(texts: List[str]) -> torch.Tensor:
    """
    E5 recommande: "query: ..." et "passage: ..."
    Ici on utilise query/passages pour optimiser.
    """
    embs = biencoder.encode(texts, normalize_embeddings=True, convert_to_tensor=True)
    return embs


def retrieve_candidates(news_title: str, docs: List[Dict], top_k: int) -> List[Tuple[int, float]]:
    """
    Retourne (index_doc, score_cosine) tri√© desc
    """
    # embeddings titres DB
    titles = [d.get("title", "") for d in docs]
    passages = [f"passage: {t}" for t in titles]
    q = embed_text_e5([f"query: {news_title}"])[0]
    P = embed_text_e5(passages)

    # cosine (car normalis√©)
    scores = (P @ q).cpu().numpy().tolist()

    ranked = sorted(list(enumerate(scores)), key=lambda x: x[1], reverse=True)
    return ranked[:top_k]


# =========================
# CROSS-ENCODER DECISION
# =========================
def cross_score_pairs(news_title: str, docs: List[Dict], idxs: List[int]) -> List[Tuple[int, float]]:
    pairs = [(news_title, docs[i]["title"]) for i in idxs]
    scores = cross.predict(pairs).tolist()
    return list(zip(idxs, scores))


# =========================
# LLM TIE-BREAKER (OUI/NON)
# =========================
def llm_same_event(news_title: str, candidate_title: str) -> bool:
    """
    Tie-breaker strict. Ne l'appelle que si n√©cessaire.
    """
    prompt = f"""
Tu es un classificateur strict.

TITRE A :
{news_title}

TITRE B :
{candidate_title}

Question :
Ces deux titres d√©crivent-ils le M√äME √âV√âNEMENT (m√™me action principale, m√™me objet, m√™me contexte),
m√™me si la formulation est diff√©rente ?

R√©ponds uniquement par :
OUI
ou
NON
""".strip()

    inputs = llm_tok(prompt, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        out = llm.generate(**inputs, max_new_tokens=3, do_sample=False, eos_token_id=llm_tok.eos_token_id)

    txt = llm_tok.decode(out[0], skip_special_tokens=True).upper()
    # on cherche la derni√®re r√©ponse
    return "OUI" in txt.splitlines()[-1]


# =========================
# PIPELINE FINAL
# =========================
def filter_same_event_titles(news: str, docs: List[Dict]) -> List[Dict]:
    news_title = make_pseudo_title_from_news(news)
    print("\nüì∞ NEWS (pseudo-titre) :", news_title)

    print("\nüîé √âtape 1 ‚Äî Retrieval (Bi-encoder) ...")
    cand = retrieve_candidates(news_title, docs, TOP_K_RETRIEVE)
    print(f"‚û°Ô∏è {len(cand)} candidats")

    cand_idxs = [i for i, _ in cand]
    print("\nüß† √âtape 2 ‚Äî Rerank / d√©cision (Cross-encoder) ...")
    scored = cross_score_pairs(news_title, docs, cand_idxs)
    scored.sort(key=lambda x: x[1], reverse=True)

    kept = []
    for i, s in scored:
        title = docs[i]["title"]
        print(f"  score={s:.3f} | {title}")

        if s >= THRESH_ACCEPT:
            kept.append(docs[i])
            continue

        if s <= THRESH_REJECT:
            continue

        # zone grise -> tie-breaker
        if USE_LLM_TIEBREAKER:
            ok = llm_same_event(news_title, title)
            print(f"    ‚Ü≥ tie-breaker LLM => {'OUI' if ok else 'NON'}")
            if ok:
                kept.append(docs[i])

    return kept


# =========================
# TEST LOCAL
# =========================
if __name__ == "__main__":
    NEWS = (
        "Le minist√®re de l‚Äô√âducation a annonc√© le lancement d‚Äôun nouveau "
        "programme national de formation des enseignants visant √† renforcer "
        "les comp√©tences num√©riques dans les √©tablissements publics."
    )

    EVENTS_DB = [
        {"date": "24 novembre 2021", "title": "Lancement d'un programme prioritaire de sensibilisation et de formation des nouveaux √©lus des collectivit√©s territoriales"},
        {"date": "13 mai 2024", "title": "M. Benmoussa annonce le prochain lancement d'une plateforme num√©rique d'enseignement √† distance de l'amazigh"},
        {"date": "08 novembre 2024", "title": "Formation initiale dans le domaine du digital: 20.000 b√©n√©ficiaires √† l‚Äôhorizon 2026 (ministre)"},
        {"date": "27 d√©cembre 2023", "title": "Le gouvernement lance un portail √©lectronique pour renforcer l'interaction avec les citoyens"},
        {"date": "14 novembre 2023", "title": "Une commission minist√©rielle sera charg√©e du traitement des probl√©matiques li√©es au statut des fonctionnaires de l‚Äô√âducation nationale"},
        {"date": "25 juillet 2023", "title": "Adoption d'un projet de d√©cret sur la vocation des √©tablissements universitaires, les cycles des √©tudes sup√©rieures et les dipl√¥mes nationaux correspondants"},
        {"date": "06 juillet 2023", "title": "Le Conseil de Gouvernement adopte un projet de loi sur l'acquisition et la mise en chantier pour la construction, la refonte ou la modification des navires de p√™ches"},
        {"date": "16 janvier 2024", "title": "L'accompagnement des startups du digital figure au centre des priorit√©s du minist√®re de la Transition num√©rique (Mme Mezzour)"},
    ]

    kept = filter_same_event_titles(NEWS, EVENTS_DB)

    print("\n‚úÖ TITRES CONSERV√âS (m√™me √©v√©nement) :")
    for d in kept:
        print("-", d["date"], "|", d["title"])
