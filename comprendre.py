#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
COUCHE ‚Äî COMPR√âHENSION D'√âV√âNEMENT (CPU ONLY)
Mod√®le : Qwen2.5-1.5B-Instruct
"""

import os
import sys
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ===============================
# FORCER CPU (IMPORTANT)
# ===============================
os.environ["CUDA_VISIBLE_DEVICES"] = ""   # d√©sactive CUDA compl√®tement
torch.set_num_threads(4)                  # ajuste selon ton CPU

MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"


def build_prompt(news: str) -> str:
    return f"""
Tu es un analyste neutre.
Explique ce que signifie cette news, sans inventer.

R√®gles :
- N'ajoute aucune information externe
- Si une info n'est pas pr√©cis√©e, dis "Non pr√©cis√©"
- R√©ponds en fran√ßais

NEWS :
\"\"\"{news}\"\"\"

Explique clairement :
- Type d'√©v√©nement
- Acteurs impliqu√©s
- Pays / lieu
- Objectif de l'action
- R√©sum√© neutre
""".strip()


def main():
    print("üß† Chargement du mod√®le de compr√©hension (CPU uniquement)...")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        device_map=None
    )
    model.to("cpu")
    model.eval()

    print("üì∞ Donne une news :")
    news = input("> ").strip()

    if not news:
        print("‚ùå News vide")
        sys.exit(1)

    prompt = build_prompt(news)

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=1024
    )

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=180,
            do_sample=False,
            temperature=0.2,
            eos_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(output[0], skip_special_tokens=True)

    print("\nüß† COMPR√âHENSION DE L‚Äô√âV√âNEMENT\n")
    print(response.split("NEWS")[-1].strip())


if __name__ == "__main__":
    main()
